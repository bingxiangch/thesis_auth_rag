# The default configuration file.
server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8001}
  cors:
    enabled: true
    allow_origins: ["*"]
    allow_methods: ["*"]
    allow_headers: ["*"]

data:
  local_data_folder: local_data/auth_RAG


database:
  type: sqlite
  url: sqlite:///./sqlite.db

system_prompt:
  default_query_system_prompt: >
    As an intelligent assistant, your role is to assist users in asking questions about patient profiles. Each source is associated with a specific user name. Please ensure that your answers to patient questions are based on the respective user's name.
llm:
  mode: local
  # Should be matching the selected model
  max_new_tokens: 512
  context_window: 3900
  tokenizer: mistralai/Mistral-7B-Instruct-v0.2

embedding:
  # Should be matching the value above in most cases
  mode: local
  ingest_mode: simple

vectorstore:
  database: chroma

local:
  prompt_style: "mistral"
  llm_hf_repo_id: TheBloke/Mistral-7B-Instruct-v0.2-GGUF
  llm_hf_model_file: mistral-7b-instruct-v0.2.Q4_K_M.gguf
  embedding_hf_model_name: BAAI/bge-small-en-v1.5

